{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "論文の付録から略語データを入手するためのプログラム"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pydantic import BaseModel\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Abbreviation(BaseModel):\n",
    "    \"\"\"略語データ\"\"\"\n",
    "\n",
    "    abbreviation: str  # 略語\n",
    "    word: str  # 原語"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = \"・\"  # 略語部分の開始文字\n",
    "END = \"：\"  # 略語部分の終了文字\n",
    "\n",
    "\n",
    "def get_data(html_path: str) -> list[Abbreviation]:\n",
    "    \"\"\"HTMLファイルごとに略語と原語を取得する関数\"\"\"\n",
    "    with open(html_path, \"r\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "    normal_size_class=\"\"\n",
    "    small_size_class=\"\"\n",
    "    for style in soup.select(\"style\"):\n",
    "        for line in style.text.split(\"\\n\"):\n",
    "            normal_match = re.match(r\"\\.(s\\d+_\\d+){font-size:16px;font-family:MS-Mincho_10-b;color:#000;}\", line)\n",
    "            if normal_match:\n",
    "                normal_size_class = normal_match.group(1)\n",
    "            small_match = re.match(r\"\\.(s\\d+_\\d+){font-size:10px;font-family:MS-Mincho_10-b;color:#000;}\", line)\n",
    "            if small_match:\n",
    "                small_size_class = small_match.group(1)\n",
    "    data: list[Abbreviation] = []\n",
    "    span_list = soup.select(\"span\")\n",
    "    is_inside = False  # 略語部分に入っているかどうか\n",
    "    for span in span_list:\n",
    "        if span.get(\"class\") is None:\n",
    "            # クラスがないspanは無視\n",
    "            continue\n",
    "        class_list: list[str] = span.get(\"class\")\n",
    "        if len(class_list) == 0:\n",
    "            # クラスがないspanは無視\n",
    "            continue\n",
    "        is_normal = len([c for c in class_list if normal_size_class == c]) > 0 # 通常の文字サイズかどうか\n",
    "        is_small = len([c for c in class_list if small_size_class == c]) > 0 # 小さな文字サイズかどうか\n",
    "        assert not(is_normal and is_small), \"文字サイズが通常と小さな文字の両方になっています\"\n",
    "        text = span.text\n",
    "        if is_inside:\n",
    "            if is_normal:\n",
    "                end_idx = text.find(END) # 終了文字を探す\n",
    "                if end_idx != -1:\n",
    "                    # 終了文字が見つかった場合は略語部分が終わる\n",
    "                    data[-1].word += text[:end_idx]\n",
    "                    data[-1].abbreviation += text[:end_idx]\n",
    "                    is_inside = False\n",
    "                else:\n",
    "                    data[-1].word += text\n",
    "                    data[-1].abbreviation += text\n",
    "            elif is_small:\n",
    "                # 小さいサイズは原語のみに含まれる\n",
    "                data[-1].word += text\n",
    "        else:\n",
    "            if text.startswith(START):\n",
    "                # 開始文字が見つかった場合は略語部分が始まる\n",
    "                data.append(Abbreviation(abbreviation=text[1:], word=text[1:]))\n",
    "                is_inside = True\n",
    "        if len(data) and \"。\" in data[-1].word:\n",
    "            # \"。\"が含まれている場合、略語を誤判定しているため、略語部分を脱出してデータを削除\n",
    "            is_inside = False\n",
    "            data.pop()\n",
    "    not_change = [d for d in data if d.abbreviation == d.word] # 略語と原語が一致しているデータ\n",
    "    if len(not_change):\n",
    "        # そのようなデータが存在することはありえないので、警告を出す\n",
    "        print(f\"略語と原語が一致しているデータがあります: {html_path}\")\n",
    "        for d in not_change:\n",
    "            print(f\"略語: {d.abbreviation}, 原語: {d.word}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Abbreviation(abbreviation='アイボリー', word='アイボリー・ナット'),\n",
       " Abbreviation(abbreviation='アスパラ', word='アスパラガス'),\n",
       " Abbreviation(abbreviation='アネロイド', word='アネロイド・バロメーター'),\n",
       " Abbreviation(abbreviation='アパートメント', word='アパートメント・ハウス'),\n",
       " Abbreviation(abbreviation='アルミ', word='アルミニウム'),\n",
       " Abbreviation(abbreviation='アルミ', word='アルミニウム・ブロンズ'),\n",
       " Abbreviation(abbreviation='アンクル', word='アンクル・エスケープメント'),\n",
       " Abbreviation(abbreviation='インバ', word='インバネス'),\n",
       " Abbreviation(abbreviation='エキス', word='エキストラクト')]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data(\"./165.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_path_list: list[str] = !find . -name \"*.html\" # 対象のHTMLファイルのリスト\n",
    "html_path_list = sorted(html_path_list)\n",
    "data: list[Abbreviation] = [] # 略語データ\n",
    "for p in html_path_list:\n",
    "    data += get_data(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump([d.dict() for d in data], open(\"abbreviation.json\", \"w\"), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
